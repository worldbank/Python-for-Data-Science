{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "**Objective:** Implement key steps in a supervised learning work-flow with SciKitLearn.\n",
    "\n",
    "**Methods to cover:** Feature engineering (dummy variables), train-test split, fitting decision trees, adjust hyper-parameters, grid-search and cross-validation, confusion matrix, ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image('data/5_steps.png', width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example 1: Predicting mode of travel to work\n",
    "\n",
    "For a workshop in November 2018, participants filled out a survey (56 responses) based on the following fictional scenario:\n",
    "\n",
    "**A new cohort of 100 staff have just been hired, and will start after Thanksgiving. We need to decide how many bike racks or parking spaces to build for them. To do so, we'll use data on current staff to build a machine learning model. For fun, we'll also predict whether staff prefer classical music or heavy metal.**\n",
    "\n",
    "The data was collected on Google Forms and downloaded as a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/worldbank/Python-for-Data-Science/master/week%208/data/responses2.csv'\n",
    "\n",
    "df = pd.read_csv(url, skiprows = 1, names = ['gender','household_type','state','favorite_food','exercise_regime','mode_of_travel','arrival_time','music_choice','purchase_history','age'])\n",
    "df = df[['gender','household_type','state','favorite_food','exercise_regime','purchase_history','age','music_choice','arrival_time','mode_of_travel']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our respondents are from a broad set of ages\n",
    "\n",
    "pd.Series(df.age.value_counts(sort=False),\n",
    "          index=['Under 25','25-34','35-44','45-54','55+']).plot(kind='bar',title='Number of respondents by age');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly more women than men. The majority live in DC.\n",
    "\n",
    "fig,axes = plt.subplots(nrows=1,ncols=2)\n",
    "pies = ['gender','state']\n",
    "\n",
    "for n,i in enumerate(pies):\n",
    "    ax = axes.flatten()[n]\n",
    "    df[i].value_counts().plot(kind='pie',autopct='%1.0f%%',ax=ax)\n",
    "    ax.set(ylabel='',title=i)\n",
    "\n",
    "fig.suptitle(\"Demographic questions\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the target variables (music choice, mode of transport).\n",
    "fig,axes = plt.subplots(1,2,sharey=True)\n",
    "df.mode_of_travel.value_counts().plot(kind='bar',ax=axes[1],title='Mode of transport')\n",
    "df.music_choice.value_counts().plot(kind='bar',ax=axes[0],title='Music choice');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a machine-readable set of features\n",
    "* We have one main continuous variable: age\n",
    "* And many categorical variables (eg State = DC, Maryland or Virginia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For age, replace the current strings (eg. '23-34') with the midpoint.\n",
    "df['age'] = df.age.map({'Under 25':20,'25-34':29.5,'35-44':39.5,'45-54':49.5,'55+':60})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a categorical variable like State machine-readable, use one-hot encoding\n",
    "df.state[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform gender, household_type, state, food and exercise to dummies\n",
    "\n",
    "for feature in ['gender','household_type','state','favorite_food','exercise_regime']:\n",
    "    dummy_feature = pd.get_dummies(df[feature])\n",
    "    df = pd.concat([df,dummy_feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the purchases data from string (eg. 'yoga mat, theater tickets') to a list.  \n",
    "\n",
    "df.purchase_history = df.purchase_history.astype(str)\n",
    "df.purchase_history = [cell.split(',') for cell in df.purchase_history]\n",
    "\n",
    "# Create dummies for purchase history\n",
    "\n",
    "items = ['Theater tickets','Yoga mat','Bluetooth speaker / headphones',\n",
    "             'Gardening or home improvement gear',' Presents for my kids']\n",
    "\n",
    "for item in items:\n",
    "    df[item] = [item in cell for cell in df.purchase_history]\n",
    "    df[item] = df[item].map({False:0,True:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our target variables are currently strings\n",
    "df.mode_of_travel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the TRANSPORT and MUSIC_CHOICE columns (our target variables) to 0s/1s\n",
    "\n",
    "y_transport = df.mode_of_travel.map({'Train':0,'Car':0,'Walk or bike':1})\n",
    "y_music = df.music_choice.map({'Classical music':0,'Heavy metal / rock':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill gaps in the age column with the mean\n",
    "\n",
    "df.age.fillna(value = df.age.mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive features are typically denoted X\n",
    "# Target variable is typically denoted Y\n",
    "\n",
    "# y = f(X) where X is a matrix comprising features x1 to Xn.\n",
    "\n",
    "features_to_use = ['Theater tickets', 'Yoga mat', 'Bluetooth speaker / headphones', 'Gardening or home improvement gear',\n",
    "       ' Presents for my kids', 'age', 'Female', 'Male',\n",
    "       'I live by myself or with housemates.', 'I live with my family.', 'DC',\n",
    "       'Maryland', 'Virginia', 'Home cooking', 'Mexican', 'Steak', 'Sushi',\n",
    "       'Vegetarian', 'I play team sports / run races.',\n",
    "       'Keep fit through jogging or gym.','Netflix is my most strenuous exercise.']\n",
    "\n",
    "X = df[features_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head(2) # Processed set of features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build decision tree model\n",
    "**4.1 TRANSPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our training and test sets, we could just take the first 70% and the last 30% of rows respectively\n",
    "n_rows = len(df)\n",
    "index_70_percent = int(n_rows * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:index_70_percent]\n",
    "X_test = X[index_70_percent:]\n",
    "y_train = y_transport[:index_70_percent]\n",
    "y_test = y_transport[index_70_percent:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train_test_split function does it for us, with randomization.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_transport,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataframes are the right size\n",
    "\n",
    "print(\"TRAINING SET\")\n",
    "print(\"X_train (features for the model to learn from): {} rows, {} columns\".format(X_train.shape[0], X_train.shape[1]))\n",
    "print(\"y_train (labels for the model to learn from): {} rows\\n\".format(y_train.shape[0]))\n",
    "print(\"TEST SET\")\n",
    "print(\"X_test (features to test the model's accuracy against): {} rows, {} columns\".format(X_test.shape[0], X_test.shape[1]))\n",
    "print(\"y_test (labels to test the model's accuracy with): {} rows\".format(y_test.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a classifier:\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3,min_samples_leaf=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it with the training data:\n",
    "\n",
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize, paste output file into www.webgraphviz.com\n",
    "\n",
    "#export_graphviz(tree_transport, out_file='tree.dot', \n",
    "#                feature_names = X,\n",
    "#                class_names = ['Train/car','walk/bike'],\n",
    "#                rounded = True, proportion = False, \n",
    "#                precision = 2, filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('data/tree1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 MUSIC TASTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_music, X_test_music, y_train_music, y_test_music = train_test_split(X,y_music,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_music = DecisionTreeClassifier(max_depth=3)\n",
    "tree_music.fit(X_train_music,y_train_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export_graphviz(music_tree, out_file='tree2.dot', \n",
    "#                feature_names = X,\n",
    "#                class_names = ['Classical','Metal'],\n",
    "#                rounded = True, proportion = False, \n",
    "#                precision = 2, filled = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('data/tree2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Predict against test set, evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_transport = tree.predict(X_test)\n",
    "\n",
    "print(\"TRANSPORT: First 10 predictions: {}\".format(predictions_transport[:10].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the predicted value equal the actual values?\n",
    "n_samples = len(predictions_transport)\n",
    "n_correct = sum(predictions_transport == y_test)\n",
    "accuracy = n_correct / n_samples * 100\n",
    "\n",
    "print(\"TRANSPORT - We predicted {} right out of {} examples. That's a {:.1f} % accuracy rate.\".format(\n",
    "    n_correct,n_samples,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy for music\n",
    "\n",
    "predictions_music = tree_music.predict(X_test_music)\n",
    "\n",
    "print(\"MUSIC - We predicted {} right out of {} examples. That's a {:.1f} % accuracy rate.\".format(\n",
    "    sum(predictions_music == y_test_music),n_samples,sum(predictions_music == y_test_music)/n_samples*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Note that the transport model addresses an easy task, whereas predicting musical taste seems difficult. What could boost the prediction accuracy? Two approaches: get more data to train your models on, or improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example 2: Predict building earthquake damage from Nepal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a currently on-going challenge hosted by DrivenData (https://www.drivendata.org/competitions/57/nepal-earthquake/data/).\n",
    "\n",
    "**Based on aspects of building location and construction, your goal is to predict the level of damage to buildings caused by the 2015 Gorkha earthquake in Nepal.**\n",
    "\n",
    "The data was collected through surveys by the Central Bureau of Statistics that work under the National Planning Commission Secretariat of Nepal. This survey is one of the largest post-disaster datasets ever collected, containing valuable information on earthquake impacts, household conditions, and socio-economic-demographic statistics.\n",
    "\n",
    "Prediction variable: `damage_grade`:\n",
    "* 1 represents low damage\n",
    "* 2 represents a medium amount of damage\n",
    "* 3 represents almost complete destruction\n",
    "\n",
    "Selected features:\n",
    "* `count_floors_pre_eq`: number of floors before the earthquake\n",
    "* `age`: age of the building in years\n",
    "* `area`: normalized area of the building footprint\n",
    "* `foundation type`: the type of foundation used when building\n",
    "* `has_superstructure_bamboo`: flag variable indicating the superstructure was made of bamboo\n",
    "* `has_superstructure_rc_engineered`: flag variable indicating the superstructure was made of reinforced concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('http://localnepaltoday.com/wp-content/uploads/2015/08/image9.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import data, create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_X = 'https://github.com/worldbank/Python-for-Data-Science/blob/master/week%208/data/train_values.csv?raw=true'\n",
    "#url_y = 'https://github.com/worldbank/Python-for-Data-Science/blob/master/week%208/data/train_labels.csv?raw=true'\n",
    "\n",
    "X = pd.read_csv('data/train_values.csv')\n",
    "y = pd.read_csv('data/train_labels.csv', usecols = ['damage_grade'])\n",
    "\n",
    "#X = pd.read_csv(url_X)\n",
    "#y = pd.read_csv(url_y, usecols = ['damage_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset has {} rows and {} columns.\".format(*X.shape))\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_make_dummies_from = ['land_surface_condition',\n",
    "'foundation_type',\n",
    "'roof_type',\n",
    "'ground_floor_type',\n",
    "'other_floor_type',\n",
    "'position',\n",
    "'plan_configuration',\n",
    "'legal_ownership_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in columns_to_make_dummies_from:\n",
    "        X.append(pd.get_dummies(X[column]))\n",
    "\n",
    "X = X.drop(columns = columns_to_make_dummies_from)\n",
    "X = X.drop(columns = 'building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's group together 'low' and 'medium' damage categories, making this a binary classification problem.\n",
    "\n",
    "y = pd.Series(y.damage_grade.map({3:1, 2:0, 1:0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB SESSION:\n",
    "\n",
    "**1. Conduct exploratory visualization** of the earthquake damage data.\n",
    "\n",
    "You may want to answer questions like: (i) what's the distribution of building age, height and floorspace; (ii) are the two classes of our outcome variable evenly balanced; (iii) was severe damage more frequent for some categories of buildings?\n",
    "\n",
    "Use Pandas plotting functions such as `df.plot(kind = 'bar, barh, pie')` or `df.hist()`.\n",
    "\n",
    "\n",
    "**2. Split the data into training and test sets.**\n",
    "\n",
    "You should feed in `X` and `y` and return four outputs.\n",
    "\n",
    "\n",
    "**3. Fit a decision tree** (call it `tree`). Then **fit a random forest** with 10 trees (call it `rf`).\n",
    "\n",
    "This requires RandomForestClassifier, specifying the parameter n_estimators for number of trees. Explore the other hyper-parameters available.\n",
    "\n",
    "If fitting the full dataset takes too long, use a subset of the data (eg. 1000 rows).\n",
    "    \n",
    "    \n",
    "**4. Predict against the test set** and see what percent of the predictions were correct.\n",
    "\n",
    "Try manually adjusting your `rf`'s hyper-parameters (eg. number of trees, depth of trees). See how performance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 10)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Predict against test set, evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_tree = tree.predict(X_test)\n",
    "sum(predictions_tree == y_test) / len(y_test)\n",
    "\n",
    "# or use the built in .score() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model tuning\n",
    "Three ways to make your model better:\n",
    "* More data\n",
    "* Better model\n",
    "* Better data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy versus amount of training data\n",
    "n_rows = np.linspace(1000, X.shape[0], 10).astype(int)\n",
    "accuracy_list = []\n",
    "\n",
    "for n in n_rows:\n",
    "    rf = RandomForestClassifier(n_estimators = 5)\n",
    "    rf.fit(X_train[:n],y_train[:n])\n",
    "    preds = rf.predict(X_test)\n",
    "    accuracy = sum(preds == y_test) / len(y_test)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "f, ax = plt.subplots()\n",
    "sns.lineplot(x = n_rows, y = accuracy_list, color = 'b', ax = ax)\n",
    "plt.title('Accuracy versus number of rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy versus number of trees\n",
    "\n",
    "def evaluate_accuracy(model):\n",
    "    ''' Returns in-sample accuracy, out-of-sample accuracy'''\n",
    "    return(model.score(X_train, y_train),\n",
    "          model.score(X_test, y_test))\n",
    "\n",
    "IS_accuracy_list = []\n",
    "OS_accuracy_list = []\n",
    "OS_dummy_accuracy_list = []\n",
    "n_estimators = np.arange(1, 10, 2)\n",
    "nrows = 1000\n",
    "\n",
    "for i in n_estimators:\n",
    "    rf = RandomForestClassifier(n_estimators = i)\n",
    "    dc = DummyClassifier(strategy = 'most_frequent')\n",
    "    rf.fit(X_train[:nrows],y_train[:nrows])\n",
    "    dc.fit(X_train[:nrows],y_train[:nrows])\n",
    "    IS_accuracy, OS_accuracy = evaluate_accuracy(rf)\n",
    "    IS_accuracy_list.append(IS_accuracy)\n",
    "    OS_accuracy_list.append(OS_accuracy)\n",
    "    OS_dummy_accuracy_list.append(evaluate_accuracy(dc)[1])\n",
    "    \n",
    "f, ax = plt.subplots()\n",
    "sns.lineplot(x = n_estimators, y = IS_accuracy_list, color = 'b', ax = ax, label = \"in-sample\")\n",
    "sns.lineplot(x = n_estimators, y = OS_accuracy_list, color = 'r', ax = ax, label = \"out-of-sample\")\n",
    "sns.lineplot(x = n_estimators, y = OS_dummy_accuracy_list, color = 'y', ax = ax, label = \"dummy\")\n",
    "\n",
    "plt.title('Accuracy versus number of trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Search for best parameters with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators':np.arange(2,42,10),'max_depth':[3,5],'min_samples_leaf':[3,5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search = GridSearchCV(RandomForestClassifier(),param_grid,cv=5)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The best parameters were: {}\".format(grid_search.best_params_))\n",
    "print(\"The best model classified {} percent of examples correctly.\".format((grid_search.best_score_*100).round(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance = pd.Series(rf.feature_importances_,index=X.columns)\n",
    "feat_importance.nlargest(8).plot(kind='barh',title='Earthquake damage: Feature importances',colors='b')\n",
    "plt.gca().invert_yaxis();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1 Build several models to compare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train[:5000], y_train[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_small = RandomForestClassifier(n_estimators = 5, max_depth = 5)\n",
    "rf_small.fit(X_train[:10000],y_train[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_full = RandomForestClassifier(n_estimators = 35)\n",
    "rf_full.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators = 35)\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.2 Confusion matrix**\n",
    "\n",
    "The predicted classes are represented in the columns of the matrix; the actual classes are in the rows of the matrix. This gives four cases as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Predicted: Class 0':['True Negative','False Negative'],\n",
    "                        'Predicted: Class 1':['False Positive','False Negative']},\n",
    "            index = ['Actual: Class 0', 'Actual: Class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = rf_full.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cm, columns = ['Predicted: Moderate damage', 'Predicted: Severe damage'],\n",
    "            index = ['Actual: Moderate damage', 'Actual: Severe damage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3 Precision, recall and F1 score**\n",
    "\n",
    "Accuracy answers the question: what proportion of predictions were correct:\n",
    "\n",
    "$$accuracy = \\frac{TP+TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "\n",
    "Precision: When it predicts the positive result, how often is it correct?\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Recall: Out of the positive examples, what proportion did it guess correctly?\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 ROC Curve\n",
    "\n",
    "Note: you can always change your decision threshold when guessing positive.\n",
    "\n",
    "By default, we class an example as positive when predicted probability (positive) > 50%.\n",
    "You can adjust the sensitivity to whatever threshold you want.\n",
    "\n",
    "A 90% probability threshold would get you more false negatives; a 30% threshold would get you a lot of false positives.\n",
    "\n",
    "Plot this trade-off on the ROC curve. Classifiers that hug the top-left corner are preferable (many true positives, few false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_tree = tree.predict_proba(X_test)[:,1]\n",
    "y_pred_prob_rf_small = rf_small.predict_proba(X_test)[:,1]\n",
    "y_pred_prob_rf_full = rf_full.predict_proba(X_test)[:,1]\n",
    "y_pred_prob_gb = gb.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr1, tpr1, thresholds1 = roc_curve(y_test, y_pred_prob_tree)\n",
    "fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred_prob_rf_small)\n",
    "fpr3, tpr3, thresholds3 = roc_curve(y_test, y_pred_prob_rf_full)\n",
    "fpr4, tpr4, thresholds4 = roc_curve(y_test, y_pred_prob_gb)\n",
    "\n",
    "# create plot\n",
    "f,ax = plt.subplots(figsize = [10,10])\n",
    "plt.plot(fpr1, tpr1, label='tree')\n",
    "plt.plot(fpr2, tpr2, label='rf_small')\n",
    "plt.plot(fpr3, tpr3, label='rf_full')\n",
    "plt.plot(fpr4, tpr4, label='gradient boosting')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Nepal earthquake damage: ROC Curve')\n",
    "#plt.xlim([-0.02, 1])\n",
    "#plt.ylim([0, 1.02])\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "APIs_geospatial",
   "language": "python",
   "name": "apis_geospatial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
